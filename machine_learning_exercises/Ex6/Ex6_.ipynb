{"nbformat_minor": 2, "cells": [{"source": "Neste rel\u00c3\u00a1torio visamos treinar um algoritmo de regress\u00c3\u00a3o com base em 50.000 dados com 77 dimens\u00c3\u00b5es usando a m\u00c3\u00a9trica MAE, erro absoluto m\u00c3\u00a9dio. Primeiramente diminuiremos as dimens\u00c3\u00b5es dos dados com PCA, ap\u00c3\u00b3s, treinaremos diversos regressores para, enfim, realizar um ensemble e reportar as sa\u00c3\u00addas esperadas.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 133, "cell_type": "code", "source": "# Includes\nfrom   sklearn.gaussian_process import GaussianProcessRegressor\nfrom   sklearn.model_selection  import GridSearchCV, StratifiedKFold, train_test_split\nfrom   sklearn.neural_network   import MLPRegressor\nfrom   sklearn.decomposition    import PCA\nfrom   sklearn.preprocessing    import scale\nfrom   sklearn.linear_model     import LinearRegression\nfrom   sklearn.neighbors        import KNeighborsRegressor\nfrom   sklearn.ensemble         import RandomForestRegressor, GradientBoostingRegressor\nfrom   sklearn.metrics          import mean_absolute_error\nfrom   sklearn.utils            import resample\nfrom   sklearn.svm              import LinearSVR\n\nimport pandas as pd\nimport numpy  as np\nimport warnings\nimport math\nimport gc\n\n#warnings.filterwarnings(\"ignore\")", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 174, "cell_type": "code", "source": "# L\u00c3\u00aa os dados do arquivo\ndef readIt():\n    from IPython.display import display\n    df = pd.io.parsers.read_csv('ex6-train.csv')\n    y = df.pop('V78').as_matrix()\n    x = scale(df.as_matrix());\n    display(df.head())\n    return x, y\n\nx, y = readIt()\ndata_size = len(x)", "outputs": [{"output_type": "display_data", "data": {"text/plain": "         V1        V2       V3        V4        V5        V6        V7  \\\n0 -0.325031 -0.311748 -0.31805 -0.315876 -0.317389 -0.327516 -0.324894   \n1 -0.320399 -0.314061 -0.31805 -0.315876 -0.318975 -0.329010 -0.320431   \n2 -0.322715 -0.316374 -0.31805 -0.315876 -0.318975 -0.323033 -0.324894   \n3 -0.322715 -0.316374 -0.31805 -0.314137 -0.318975 -0.329010 -0.324894   \n4 -0.325031 -0.314061 -0.31805 -0.315876 -0.317389 -0.326021 -0.321919   \n\n         V8        V9       V10    ...          V68       V69       V70  \\\n0 -0.360825 -0.340404 -0.343175    ...    -0.071290 -0.095431 -0.090428   \n1 -0.350666 -0.340404 -0.343175    ...    -0.782664 -0.793595 -0.090428   \n2 -0.355746 -0.345350 -0.343175    ...    -0.782664 -0.095431 -0.090428   \n3 -0.355746 -0.345350 -0.343175    ...    -0.782664 -0.793595 -0.090428   \n4 -0.360825 -0.340404 -0.343175    ...    -0.071290 -0.095431 -0.090428   \n\n        V71       V72       V73       V74       V75       V76       V77  \n0 -0.325432 -0.312178 -0.318445 -0.316260 -0.317772 -0.327894 -0.325299  \n1 -0.320809 -0.314487 -0.318445 -0.316260 -0.319357 -0.329387 -0.320842  \n2 -0.323120 -0.316796 -0.318445 -0.316260 -0.319357 -0.323417 -0.325299  \n3 -0.323120 -0.316796 -0.318445 -0.314523 -0.319357 -0.329387 -0.325299  \n4 -0.325432 -0.314487 -0.318445 -0.316260 -0.317772 -0.326402 -0.322327  \n\n[5 rows x 77 columns]", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>V1<\/th>\n      <th>V2<\/th>\n      <th>V3<\/th>\n      <th>V4<\/th>\n      <th>V5<\/th>\n      <th>V6<\/th>\n      <th>V7<\/th>\n      <th>V8<\/th>\n      <th>V9<\/th>\n      <th>V10<\/th>\n      <th>...<\/th>\n      <th>V68<\/th>\n      <th>V69<\/th>\n      <th>V70<\/th>\n      <th>V71<\/th>\n      <th>V72<\/th>\n      <th>V73<\/th>\n      <th>V74<\/th>\n      <th>V75<\/th>\n      <th>V76<\/th>\n      <th>V77<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>-0.325031<\/td>\n      <td>-0.311748<\/td>\n      <td>-0.31805<\/td>\n      <td>-0.315876<\/td>\n      <td>-0.317389<\/td>\n      <td>-0.327516<\/td>\n      <td>-0.324894<\/td>\n      <td>-0.360825<\/td>\n      <td>-0.340404<\/td>\n      <td>-0.343175<\/td>\n      <td>...<\/td>\n      <td>-0.071290<\/td>\n      <td>-0.095431<\/td>\n      <td>-0.090428<\/td>\n      <td>-0.325432<\/td>\n      <td>-0.312178<\/td>\n      <td>-0.318445<\/td>\n      <td>-0.316260<\/td>\n      <td>-0.317772<\/td>\n      <td>-0.327894<\/td>\n      <td>-0.325299<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>-0.320399<\/td>\n      <td>-0.314061<\/td>\n      <td>-0.31805<\/td>\n      <td>-0.315876<\/td>\n      <td>-0.318975<\/td>\n      <td>-0.329010<\/td>\n      <td>-0.320431<\/td>\n      <td>-0.350666<\/td>\n      <td>-0.340404<\/td>\n      <td>-0.343175<\/td>\n      <td>...<\/td>\n      <td>-0.782664<\/td>\n      <td>-0.793595<\/td>\n      <td>-0.090428<\/td>\n      <td>-0.320809<\/td>\n      <td>-0.314487<\/td>\n      <td>-0.318445<\/td>\n      <td>-0.316260<\/td>\n      <td>-0.319357<\/td>\n      <td>-0.329387<\/td>\n      <td>-0.320842<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>-0.322715<\/td>\n      <td>-0.316374<\/td>\n      <td>-0.31805<\/td>\n      <td>-0.315876<\/td>\n      <td>-0.318975<\/td>\n      <td>-0.323033<\/td>\n      <td>-0.324894<\/td>\n      <td>-0.355746<\/td>\n      <td>-0.345350<\/td>\n      <td>-0.343175<\/td>\n      <td>...<\/td>\n      <td>-0.782664<\/td>\n      <td>-0.095431<\/td>\n      <td>-0.090428<\/td>\n      <td>-0.323120<\/td>\n      <td>-0.316796<\/td>\n      <td>-0.318445<\/td>\n      <td>-0.316260<\/td>\n      <td>-0.319357<\/td>\n      <td>-0.323417<\/td>\n      <td>-0.325299<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>-0.322715<\/td>\n      <td>-0.316374<\/td>\n      <td>-0.31805<\/td>\n      <td>-0.314137<\/td>\n      <td>-0.318975<\/td>\n      <td>-0.329010<\/td>\n      <td>-0.324894<\/td>\n      <td>-0.355746<\/td>\n      <td>-0.345350<\/td>\n      <td>-0.343175<\/td>\n      <td>...<\/td>\n      <td>-0.782664<\/td>\n      <td>-0.793595<\/td>\n      <td>-0.090428<\/td>\n      <td>-0.323120<\/td>\n      <td>-0.316796<\/td>\n      <td>-0.318445<\/td>\n      <td>-0.314523<\/td>\n      <td>-0.319357<\/td>\n      <td>-0.329387<\/td>\n      <td>-0.325299<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>-0.325031<\/td>\n      <td>-0.314061<\/td>\n      <td>-0.31805<\/td>\n      <td>-0.315876<\/td>\n      <td>-0.317389<\/td>\n      <td>-0.326021<\/td>\n      <td>-0.321919<\/td>\n      <td>-0.360825<\/td>\n      <td>-0.340404<\/td>\n      <td>-0.343175<\/td>\n      <td>...<\/td>\n      <td>-0.071290<\/td>\n      <td>-0.095431<\/td>\n      <td>-0.090428<\/td>\n      <td>-0.325432<\/td>\n      <td>-0.314487<\/td>\n      <td>-0.318445<\/td>\n      <td>-0.316260<\/td>\n      <td>-0.317772<\/td>\n      <td>-0.326402<\/td>\n      <td>-0.322327<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n<p>5 rows \u00c3\u0097 77 columns<\/p>\n<\/div>"}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# Preprocessamento\n## PCA", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 151, "cell_type": "code", "source": "def pcaIt(x):\n    pca = PCA(n_components=0.85)\n    pca.fit(x)\n    print(\"Ap\u00c3\u00b3s o PCA temos\", pca.n_components_,\"componentes\")\n    return pca.transform(x)\n    \nx_reduced = pcaIt(x)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Ap\u00c3\u00b3s o PCA temos 11 componentes\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# An\u00c3\u00a1lise visual", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "def plotIt():\n    import matplotlib.pyplot as plt\n    \n    def plot(i, x, y, ncols, fig):\n        if i % ncols == 0:\n            fig = plt.figure(figsize=(15,2))\n        ax = fig.add_subplot(1, ncols, 1+(i%ncols))\n        ax.grid(color='lightgray', linestyle='--', linewidth=1)\n        x_, y_ = [[*x] for x in zip(*sorted(zip(x,y)))]\n        ax.plot(x_, y_, '.')\n        ax.set_title(\"Componente {}\".format(i+1))\n        if i % ncols == ncols-1:\n            plt.show()\n        return fig\n\n    transposed = np.transpose(x)\n    transposed_reduced = np.transpose(x_reduced)\n    \n    fig = None\n    ncols = 5\n    print(\"Componentes do dado normalizado\")\n    for i in range(len(transposed)):\n        fig = plot(i, transposed[i], y, ncols, fig)\n    plt.show()\n\n    print(\"Componentes do dado ap\u00c3\u00b3s o PCA\")\n    for i in range(len(transposed_reduced[:])):\n        fig = plot(i, transposed_reduced[i], y, ncols, fig)\n    plt.show()\n    \nplotIt()", "outputs": [], "metadata": {"scrolled": false, "collapsed": false, "editable": true, "deletable": true}}, {"source": "Analisar as componentes isoladamente pode gerar m\u00c3\u00a1s interpreta\u00c3\u00a7\u00c3\u00b5es, mas podemos ver que algumas t\u00c3\u00aam comportamentos bem similares, outras, bem comportadas, e ainda h\u00c3\u00a1 alguns casos, como nas componentes entre 43 e 49, onde os dados variam entre 2 valores.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "# Treinamento", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 175, "cell_type": "code", "source": "x = x_reduced\nx_smallSample, y_smallSample = resample(x, y, n_samples=500)\ngc.collect()\n\ndef mae_scorer(estimator, x, y):\n    return mean_absolute_error(y, estimator.predict(x))\n\ndef testIt(regressor, name, params, resultStr, x, y):\n    gscv = GridSearchCV(regressor, params, n_jobs=4, scoring='neg_mean_absolute_error', cv=5)\n    gscv.fit(x,y)\n    regressor = gscv.best_estimator_\n    regressor.score = mae_scorer(regressor, x, y)\n    paramList = [gscv.best_params_[name] for name in sorted(params.keys())]\n    formatParams = [name, -gscv.best_score_, regressor.score] + paramList\n    resultStr = \"{} >> Menor MAE (Fold / Total): {:.3f} / {:.3f};\\t\" + resultStr\n    print(resultStr.format(*formatParams))\n    gc.collect()\n    return regressor", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "## SVM", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 171, "cell_type": "code", "source": "def trainSVM(x, y):\n    regressor = LinearSVR(loss='epsilon_insensitive', random_state=0)\n    params = {'epsilon':[0, 0.1, 1, 10], 'C':[2**-10, 2**-5, 2**1, 2**5]}\n    resultStr = \"C: {:.3f}; Eps: {}\"\n    return testIt(regressor, \"SVM\", params, resultStr, x, y)\n\ntrainSVM(x_smallSample, y_smallSample)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "SVM >> Menor MAE (Fold / Total): 0.135 / 0.124;\tC: 0.001; Eps: 0\n"}, {"execution_count": 171, "output_type": "execute_result", "data": {"text/plain": "LinearSVR(C=0.0009765625, dual=True, epsilon=0, fit_intercept=True,\n     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n     random_state=0, tol=0.0001, verbose=0)"}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "## GBM", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 128, "cell_type": "code", "source": "def trainGBM(x, y):\n    regressor = GradientBoostingRegressor(loss='lad', random_state=0)\n    params = {'learning_rate':[0.1, 0.05], 'max_depth':[3, 5, 7], 'n_estimators':[30, 70, 100]}\n    resultStr = \"L.R.: {}; Max Depth: {}; N Est.: {}\"\n    return testIt(regressor, \"GBM\", params, resultStr, x, y)\n\ntrainGBM(x_smallSample, y_smallSample)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Menor MAE (Fold / Total): 0.114 / 0.044;\tL.R.: 0.1;\tMax Depth: 7;\tN Est.: 100\n"}, {"execution_count": 128, "output_type": "execute_result", "data": {"text/plain": "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='lad', max_depth=7, max_features=None,\n             max_leaf_nodes=None, min_impurity_split=1e-07,\n             min_samples_leaf=1, min_samples_split=2,\n             min_weight_fraction_leaf=0.0, n_estimators=100,\n             presort='auto', random_state=0, subsample=1.0, verbose=0,\n             warm_start=False)"}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "## RF", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 129, "cell_type": "code", "source": "def trainRF(x, y):\n    regressor = RandomForestRegressor(criterion='mae', random_state=0)\n    params = {'n_estimators': [50, 100, 150], 'max_depth': [3, 5, 7]}\n    resultStr = \"Max Depth: {}; n_estimators: {}\"\n    return testIt(regressor, \"RF\", params, resultStr, x, y)\n\ntrainRF(x_smallSample, y_smallSample)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Menor MAE (Fold / Total): 0.117 / 0.082;\tMax Depth: 3;\tn_estimators: 50\n"}, {"execution_count": 129, "output_type": "execute_result", "data": {"text/plain": "RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_split=1e-07, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=50, n_jobs=1, oob_score=False, random_state=0,\n           verbose=0, warm_start=False)"}, "metadata": {}}], "metadata": {"scrolled": true, "collapsed": false, "editable": true, "deletable": true}}, {"source": "## RN", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 130, "cell_type": "code", "source": "def trainRN(x, y):\n    regressor = MLPRegressor(random_state=0, alpha=1e+20)\n    params = { 'hidden_layer_sizes':[(80,), (100,), (120,)], 'activation' : ['identity', 'logistic', 'tanh', 'relu']}\n    resultStr = \"hidden_layer_size: {}; activation: {}\"\n    return testIt(regressor, \"RN\", params, resultStr, x, y)\n\ntrainRN(x_smallSample, y_smallSample)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Menor MAE (Fold / Total): 0.385 / 0.384;\thidden_layer_size: logistic; \tactivation: (120,)\n"}, {"execution_count": 130, "output_type": "execute_result", "data": {"text/plain": "MLPRegressor(activation='logistic', alpha=1e+20, batch_size='auto',\n       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n       hidden_layer_sizes=(120,), learning_rate='constant',\n       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n       warm_start=False)"}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "## KNN", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 170, "cell_type": "code", "source": "def trainKNN(x, y):\n    regressor = KNeighborsRegressor(p=1)\n    params = { 'n_neighbors':[1, 5, 11, 15, 21, 25], 'weights':['distance', 'uniform']}\n    resultStr = \"n_neighbors: {}; weights: {}\"\n    return testIt(regressor, \"KNN\", params, resultStr, x, y)\n\ntrainKNN(x_smallSample, y_smallSample)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "KNN >> Menor MAE (Fold / Total): 0.212 / 0.000;\tn_neighbors: 11; weights: distance\n"}, {"execution_count": 170, "output_type": "execute_result", "data": {"text/plain": "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n          metric_params=None, n_jobs=1, n_neighbors=11, p=1,\n          weights='distance')"}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "## Gaussian", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 132, "cell_type": "code", "source": "def trainGaussian(x, y): \n    regressor = GaussianProcessRegressor(random_state=0)\n    params = { 'alpha':[1e-9, 1e-6, 1e-3, 1]}\n    resultStr = \"alpha: {}\"\n    return testIt(regressor, \"Gaussian\", params, resultStr, x, y)\n\ntrainGaussian(x_smallSample, y_smallSample)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Menor MAE (Fold / Total): 0.214 / 0.128;\talpha: 1\n"}, {"execution_count": 132, "output_type": "execute_result", "data": {"text/plain": "GaussianProcessRegressor(alpha=1, copy_X_train=True, kernel=None,\n             n_restarts_optimizer=0, normalize_y=False,\n             optimizer='fmin_l_bfgs_b', random_state=0)"}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# Ensemble", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 172, "cell_type": "code", "source": "# Os c\u00c3\u00b3digos para ensemble s\u00c3\u00a3o grandemente inspirados de uma resposta\n# creditada \u00c3\u00a0 Shahar Azulay & Ami Tavory. Acessado em 17/06/2017\n# url: https://stackoverflow.com/a/35170149/3171285\n\n# Tamanhos arbitrariamente escolhidos conforme capacidade da m\u00c3\u00a1quina\nsample_size = data_size*0.04\nsample_size_hp = sample_size*0.8\n\n# Inicializa o vetor de predi\u00c3\u00a7\u00c3\u00b5es para y\ny_pred = np.zeros(n_regressors)\n\n# Separa treino e teste para checar acur\u00c3\u00a1cia da uni\u00c3\u00a3o dos 6 m\u00c3\u00a9todos\ni_tr, i_ts = train_test_split(list(range(data_size)), train_size=sample_size, test_size=sample_size//4)\nx_tr, x_ts, y_tr, y_ts = x[i_tr], x[i_ts], y[i_tr], y[i_ts]\n    \n# Subsample do treino para descobrir hiperpar\u00c3\u00a2metros\ntrain_hp = resample(x_tr, y_tr, n_samples=sample_size_hp)\n\n# Gera objetos dos regressores com busca de hiperpar\u00c3\u00a2metro\nregressors = [trainSVM(*train_hp), \n              #trainGBM(*train_hp), \n              #trainRF(*train_hp), \n              #trainRN(*train_hp), \n              trainKNN(*train_hp), \n              #trainGaussian(*train_hp)\n             ]\nn_regressors = len(regressors)\n\n# Gera a predi\u00c3\u00a7\u00c3\u00a3o para cada regressor\nfor i in range(n_regressors):\n    # A principal diferen\u00c3\u00a7a entre estas duas linhas \u00c3\u00a9 que a primeira tenta evitar overfitting atrav\u00c3\u00a9s de treinos\n    y_pred[i_tr, i] = Stacker(regressors[i]).fit_transform(x_tr, y_tr)[:,0] \n    y_pred[i_ts, i] = Stacker(regressors[i]).fit(x_ts, y_tr).transform(x_ts)\n    \n# Gera um regressor com base nas predi\u00c3\u00a7\u00c3\u00b5es\nu = LinearRegression().fit(y_pred[i_tr, :], y_tr)\n\n# Para cada re\nfor i in range(n_regressors):\n    \n\n# Calcula a pontua\u00c3\u00a7\u00c3\u00a3o baseada\nmae_scorer(u, y_pred[test_index, :], y[test_index])", "outputs": [{"output_type": "stream", "name": "stdout", "text": "SVM >> Menor MAE (Fold / Total): 0.096 / 0.095;\tC: 0.031; Eps: 0\nKNN >> Menor MAE (Fold / Total): 0.087 / 0.000;\tn_neighbors: 5; weights: distance\n"}], "metadata": {"collapsed": false}}, {"source": "Obs.:   O resultado esperado pelos dados de teste est\u00c3\u00a3o no arquivo ex6-result.csv\n        S\u00c3\u00a3o criadas fun\u00c3\u00a7\u00c3\u00b5es para trechos triviais para otimizar a mem\u00c3\u00b3ria dispon\u00c3\u00advel.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 173, "cell_type": "code", "source": "\nclass Stacker(object):\n    \"\"\"\n    A transformer applying fitting a predictor `pred` to data in a way\n        that will allow a higher-up predictor to build a model utilizing both this \n        and other predictors correctly.\n\n    The fit_transform(self, x, y) of this class will create a column matrix, whose \n        each row contains the prediction of `pred` fitted on other rows than this one. \n        This allows a higher-level predictor to correctly fit a model on this, and other\n        column matrices obtained from other lower-level predictors.\n\n    The fit(self, x, y) and transform(self, x_) methods, will fit `pred` on all \n        of `x`, and transform the output of `x_` (which is either `x` or not) using the fitted \n        `pred`.\n\n    Arguments:    \n        pred: A lower-level predictor to stack.\n\n        cv_fn: Function taking `x`, and returning a cross-validation object. In `fit_transform`\n            th train and test indices of the object will be iterated over. For each iteration, `pred` will\n            be fitted to the `x` and `y` with rows corresponding to the\n            train indices, and the test indices of the output will be obtained\n            by predicting on the corresponding indices of `x`.\n    \"\"\"\n    def __init__(self, pred, cv_fn=lambda x: sklearn.cross_validation.KFold().split(x)):\n        self._pred, self._cv_fn  = pred, cv_fn\n\n    def fit_transform(self, x, y):\n        x_trans = self._train_transform(x, y)\n\n        self.fit(x, y)\n\n        return x_trans\n\n    def fit(self, x, y):\n        \"\"\"\n        Same signature as any sklearn transformer.\n        \"\"\"\n        self._pred.fit(x, y)\n\n        return self\n\n    def transform(self, x):\n        \"\"\"\n        Same signature as any sklearn transformer.\n        \"\"\"\n        return self._test_transform(x)\n\n    def _train_transform(self, x, y):\n        x_trans = np.nan * np.ones((x.shape[0], 1))\n\n        all_te = set()\n        for tr, te in self._cv_fn(x):\n            all_te = all_te | set(te)\n            x_trans[te, 0] = self._pred.fit(x[tr, :], y[tr]).predict(x[te, :]) \n        if all_te != set(range(x.shape[0])):\n            warnings.warn('Not all indices covered by Stacker', sklearn.exceptions.FitFailedWarning)\n\n        return x_trans\n\n    def _test_transform(self, x):\n        return self._pred.predict(x)\n    \n    ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3.6", "name": "python36", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.0", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}