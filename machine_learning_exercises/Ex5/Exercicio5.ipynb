{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessamento\n",
    "### Faça as tarefas usuais de processameno de textos:\n",
    "    Conversão de caracteres maiúsculos para minúsculos\n",
    "    Remoção de pontuação\n",
    "    Remoção de stop words\n",
    "    Steming dos termos\n",
    "    Remoção dos termos que aparecem em um só documento ou que aparecem em todos.\n",
    "\n",
    "### Converta os textos processados em um bag of words no formato binário e no formato de term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tales/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tales/virtualenvs/ML/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import nltk, sklearn, string, re, numpy, scipy, math\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "data = load_files('./ex5-files/filesk/', encoding='utf8', shuffle=True, \n",
    "                                                          load_content=False)\n",
    "\n",
    "def stemmingTokens(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text) # Remoção de pontuação\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmer = EnglishStemmer()\n",
    "    stems = [stemmer.stem(i) for i in tokens] # Stemming dos termos\n",
    "    return stems\n",
    "\n",
    "qntDocs = 5000\n",
    "vectorizer = CountVectorizer(\n",
    "        input='filename', \n",
    "        binary=False,\n",
    "        strip_accents='unicode',\n",
    "        lowercase=True,            # Conversão para minúsculas\n",
    "        tokenizer=stemmingTokens,  # Remoção de pontuação e stemming dos termos\n",
    "        stop_words='english',      # Remoção de stop words\n",
    "        min_df=2,                  # Remoção dos termos em um só documento...\n",
    "        max_df=qntDocs-1)          # ... ou que aparecem em todos \n",
    "\n",
    "countSM = vectorizer.fit_transform(data['filenames'][:qntDocs])\n",
    "binSM = countSM.sign()\n",
    "y = data['target'][:qntDocs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divida o conjunto em  4000 documentos de treino e 1000 de teste aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(countSM, y, test_size=0.2, \n",
    "                                                                      random_state=0)\n",
    "x_trainBin, x_testBin, y_trainBin, y_testBin = train_test_split(binSM, y, \n",
    "                                                       test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2 - Naive Bayes\n",
    "### Rode o *Naive Bayes* na matriz binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média usando BernoulliNB na matriz binária: 0.744\n"
     ]
    }
   ],
   "source": [
    "binScore = BernoulliNB(binarize=None).fit(x_trainBin, y_trainBin).score(x_testBin, \n",
    "                                                                        y_testBin)\n",
    "print(\"Acurácia média usando BernoulliNB na matriz binária: {0:.3f}\".format(binScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe BernoulliNB poderia receber a matriz de frequência e criar a matriz binária internamente mas esta abordagem mostrou resultados inferiores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rode o *Naive Bayes*  na matriz de term-frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média usando MultinomialNB na matriz de frequência de termos: 0.798\n"
     ]
    }
   ],
   "source": [
    "freqScore = MultinomialNB().fit(x_train, y_train).score(x_test, y_test)\n",
    "print(\"Acurácia média usando MultinomialNB na matriz de frequência de termos: {0:.3f}\"\n",
    "                                                                   .format(freqScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser avaliado, temos cerca de 5.4% a mais de acurácia usando a matriz de term frequency no lugar da matriz binária."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA e outros classificadores\n",
    "### Rode o PCA na matriz de term-frequency mantendo 99% da variância original com o TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com 2851 palavras/componentes há variância igual à 99.00%\n"
     ]
    }
   ],
   "source": [
    "n_words = int(3000/2)\n",
    "totalVariance = 0\n",
    "while (totalVariance < 0.99):\n",
    "    n_words *= 2\n",
    "    fullSVD = TruncatedSVD(n_components=n_words, n_iter=1)\n",
    "    fullSVD.fit(countSM, y)\n",
    "\n",
    "    n_words = 0\n",
    "    totalVariance = 0\n",
    "    for variance in sorted(fullSVD.explained_variance_ratio_, reverse=True):\n",
    "        totalVariance += variance\n",
    "        n_words += 1\n",
    "        print(totalVariance, end='\\r')\n",
    "        if (totalVariance >= 0.99):\n",
    "            break\n",
    "    print(\"Com {0} palavras/componentes há variância igual à {1:.2f}%\"\n",
    "              .format(n_words, totalVariance*100))\n",
    "\n",
    "SVD = TruncatedSVD(n_components=n_words, n_iter=5)\n",
    "x_PCA = SVD.fit_transform(countSM, y)\n",
    "x_trainPCA, x_testPCA, y_trainPCA, y_testPCA = train_test_split(x_PCA, y, \n",
    "                                                        test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rode os seguintes algoritmos na matriz com o número de dimensões reduzidas, buscando hiperparâmetros.\n",
    "\n",
    "#### SVM com RBF (modo one vs all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste do SVM\n",
      "C = 2^1, gamma = 2^-10; Acurácia = 0.837000000000\n"
     ]
    }
   ],
   "source": [
    "# Conjunto de hiperparâmetros testados para otimização do SVM\n",
    "Cs = [2**-5, 2**1, 2**5, 2**10]\n",
    "gammas = [2**-15, 2**-10, 2**0, 2**5]\n",
    "\n",
    "print(\"Teste do SVM\")\n",
    "\n",
    "# Procura o melhor C e gamma com 3 folds (kernel='rbf' por padrão)\n",
    "gscv = GridSearchCV(\n",
    "            OneVsRestClassifier(SVC()), \n",
    "            {'estimator__C':Cs, 'estimator__gamma':gammas},\n",
    "            n_jobs=3\n",
    "       ).fit(x_trainPCA, y_trainPCA)\n",
    "\n",
    "C = int(math.log(gscv.best_params_[\"estimator__C\"], 2))\n",
    "gamma = int(math.log(gscv.best_params_[\"estimator__gamma\"], 2))\n",
    "svmScore = gscv.score(x_testPCA, y_testPCA)\n",
    "print('C = 2^{:d}, gamma = 2^{:d}; Acurácia = {:.12f}'.format(\n",
    "            C, \n",
    "            gamma, \n",
    "            svmScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste do GBM\n",
      "n_estimators = 100, learning_rate = 0.10; Acurácia = 0.810000000000\n"
     ]
    }
   ],
   "source": [
    "# Conjunto de hiperparâmetros testados para otimização do GBM\n",
    "n_estimators = [30, 70, 100]\n",
    "learning_rates = [0.1, 0.05]\n",
    "max_depth = [5]\n",
    "\n",
    "print(\"Teste do GBM\")\n",
    "\n",
    "# Procura os melhores hiperparâmetros 3 folds (StratifiedKFold)\n",
    "gscv = GridSearchCV(\n",
    "            GradientBoostingClassifier(), {\n",
    "                'n_estimators':n_estimators, \n",
    "                'learning_rate':learning_rates, \n",
    "                'max_depth':max_depth},\n",
    "            n_jobs=3\n",
    "       ).fit(x_trainPCA, y_trainPCA)\n",
    "\n",
    "gbmScore = gscv.score(x_testPCA, y_testPCA)\n",
    "print('n_estimators = {:3d}, learning_rate = {:.2f}; Acurácia = {:.12f}'.format(\n",
    "            gscv.best_params_[\"n_estimators\"], \n",
    "            gscv.best_params_[\"learning_rate\"], \n",
    "            gbmScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste do RF\n",
      "n_estimators = 50, max_features = 7; Acurácia = 0.564000000000\n"
     ]
    }
   ],
   "source": [
    "# Conjunto de hiperparâmetros testados para otimização do RF\n",
    "max_features = [3, 5, 7]\n",
    "n_estimators = [50, 100, 500, 1000]\n",
    "\n",
    "print(\"Teste do RF\")\n",
    "\n",
    "# Procura os melhores hiperparâmetros 3 folds (StratifiedKFold)\n",
    "gscv = GridSearchCV(\n",
    "            RandomForestClassifier(), \n",
    "            {'n_estimators':n_estimators, 'max_features':max_features},\n",
    "            n_jobs=3\n",
    "       ).fit(x_trainPCA, y_trainPCA)\n",
    "\n",
    "# Guarda a acurácia da Random Forest para os hiperparâmetros do fold atual\n",
    "rfScore = gscv.score(x_testPCA, y_testPCA)\n",
    "print('n_estimators = {:d}, max_features = {:d}; Acurácia = {:.12f}'.format(\n",
    "            gscv.best_params_[\"n_estimators\"], \n",
    "            gscv.best_params_[\"max_features\"],\n",
    "            rfScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerações Finais\n",
    "### Qual o melhor classificador dos testados?\n",
    "Como podemos ver, o SVC com RBF tem a melhor acurácia: 0.837, mas exige um pesado processamento. Abaixo, um treinamento com toda a base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=32, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0009765625, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "gscv = GridSearchCV(\n",
    "            SVC(), \n",
    "            {'C':Cs, 'gamma':gammas},\n",
    "            n_jobs=3\n",
    "       ).fit(x_PCA, y)\n",
    "print(gscv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale reforçar que talvez o ganho de acurácia não compense o tempo de processamento exigido pelo SVM. Os métodos Naive tem acurácia satisfatória e exigem muito menos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
